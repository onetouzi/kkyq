import torch
from torchvision import models, transforms
import torch.nn as nn
from PIL import Image
import os
import warnings

warnings.filterwarnings('ignore')

# ===================== æ ¸å¿ƒé…ç½® =====================
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
MODEL_PATH = 'best_book_classifier.pth'
THRESHOLD = 0.5  # åˆ†ç±»é˜ˆå€¼ï¼ˆå›¾ä¹¦å°é¢åˆ¤å®šç•Œé™ï¼‰
PREDICT_DIR = "./val"  # å¾…é¢„æµ‹å›¾ç‰‡æ‰€åœ¨æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆä¿®æ”¹è¿™é‡Œï¼ï¼‰

# å›¾ç‰‡é¢„å¤„ç†ï¼ˆå’Œè®­ç»ƒ/æ¨ç†æ—¶å®Œå…¨ä¸€è‡´ï¼‰
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# ===================== æ¨¡å‹æ„å»ºä¸åŠ è½½ï¼ˆResNet50ï¼‰ =====================
def build_model():
    """æ„å»ºResNet50æ¨¡å‹ç»“æ„ï¼ˆå’Œè®­ç»ƒæ—¶ä¸€è‡´ï¼‰"""
    model = models.resnet50(pretrained=False)
    for param in model.parameters():
        param.requires_grad = False
    num_ftrs = model.fc.in_features
    model.fc = nn.Sequential(
        nn.Linear(num_ftrs, 128),
        nn.ReLU(),
        nn.Dropout(0.5),
        nn.Linear(128, 1),
        nn.Sigmoid()
    )
    return model

def load_model():
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡"""
    try:
        model = build_model()
        checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)
        model.load_state_dict(checkpoint['model_state_dict'])
        model.to(DEVICE)
        model.eval()  # åˆ‡æ¢åˆ°æ¨ç†æ¨¡å¼ï¼ˆå…³é—­Dropoutï¼‰
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼æ¨ç†è®¾å¤‡ï¼š{DEVICE}")
        return model
    except FileNotFoundError:
        raise Exception(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼š{MODEL_PATH}")
    except KeyError as e:
        raise Exception(f"âŒ æ¨¡å‹æƒé‡ä¸åŒ¹é…ï¼š{e}ï¼ˆè¯·ç¡®è®¤æ˜¯ResNet50æ¨¡å‹ï¼‰")
    except Exception as e:
        raise Exception(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{str(e)}")

# ===================== æ ¸å¿ƒé¢„æµ‹å‡½æ•° =====================
def predict_image(model, image_path):
    """
    å•å¼ å›¾ç‰‡é¢„æµ‹
    :param model: åŠ è½½å¥½çš„æ¨¡å‹å®ä¾‹
    :param image_path: å›¾ç‰‡æ–‡ä»¶è·¯å¾„
    :return: é¢„æµ‹ç»“æœå­—å…¸
    """
    # 1. æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(image_path):
        return {"status": "error", "message": f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨", "filename": os.path.basename(image_path)}
    
    # 2. åŠ è½½å¹¶é¢„å¤„ç†å›¾ç‰‡
    try:
        image = Image.open(image_path).convert('RGB')
        img_tensor = transform(image).unsqueeze(0).to(DEVICE)  # æ·»åŠ batchç»´åº¦
    except Exception as e:
        return {
            "status": "error", 
            "message": f"å›¾ç‰‡åŠ è½½å¤±è´¥ï¼š{str(e)[:30]}", 
            "filename": os.path.basename(image_path)
        }
    
    # 3. æ¨¡å‹æ¨ç†
    with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ŒèŠ‚çœæ˜¾å­˜
        output = model(img_tensor)
        confidence = round(output.item(), 4)  # ç½®ä¿¡åº¦ï¼ˆ0~1ï¼‰
        is_book = confidence >= THRESHOLD     # æ˜¯å¦ä¸ºå›¾ä¹¦å°é¢
    
    # 4. æ„é€ ç»“æœ
    return {
        "status": "success",
        "filename": os.path.basename(image_path),
        "file_path": image_path,
        "is_book": is_book,
        "confidence": confidence,
        "threshold": THRESHOLD,
        "result_desc": "æ˜¯å›¾ä¹¦å°é¢" if is_book else "éå›¾ä¹¦å°é¢"
    }

def batch_predict_from_dir(model, dir_path):
    """
    è¯»å–æŒ‡å®šæ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰å›¾ç‰‡å¹¶æ‰¹é‡é¢„æµ‹
    :param model: åŠ è½½å¥½çš„æ¨¡å‹å®ä¾‹
    :param dir_path: å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„
    :return: æ‰¹é‡é¢„æµ‹ç»“æœåˆ—è¡¨
    """
    # 1. æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
    if not os.path.exists(dir_path):
        print(f"âŒ æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼š{dir_path}")
        return []
    
    # 2. æ”¶é›†æ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
    image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tiff')
    image_paths = []
    for file_name in os.listdir(dir_path):
        # è¿‡æ»¤éå›¾ç‰‡æ–‡ä»¶
        if file_name.lower().endswith(image_extensions):
            image_paths.append(os.path.join(dir_path, file_name))
    
    # 3. æ£€æŸ¥æ˜¯å¦æœ‰å›¾ç‰‡æ–‡ä»¶
    if len(image_paths) == 0:
        print(f"âŒ æ–‡ä»¶å¤¹ {dir_path} ä¸‹æœªæ‰¾åˆ°ä»»ä½•å›¾ç‰‡æ–‡ä»¶ï¼ˆæ”¯æŒæ ¼å¼ï¼š{image_extensions}ï¼‰")
        return []
    
    # 4. æ‰¹é‡é¢„æµ‹
    print(f"\nğŸ“ å¼€å§‹é¢„æµ‹æ–‡ä»¶å¤¹ {dir_path} ä¸‹çš„å›¾ç‰‡ï¼Œå…± {len(image_paths)} å¼ ")
    print("-" * 80)
    results = []
    for idx, img_path in enumerate(image_paths, 1):
        result = predict_image(model, img_path)
        results.append(result)
        # æ‰“å°å•å¼ ç»“æœ
        if result["status"] == "success":
            print(f"[{idx}/{len(image_paths)}] {result['filename']} â†’ {result['result_desc']}ï¼ˆç½®ä¿¡åº¦ï¼š{result['confidence']}ï¼‰")
        else:
            print(f"[{idx}/{len(image_paths)}] {result['filename']} â†’ âŒ {result['message']}")
    
    return results

# ===================== ä¸»å‡½æ•°ï¼ˆè‡ªåŠ¨è¯»å–æ–‡ä»¶å¤¹é¢„æµ‹ï¼‰ =====================
if __name__ == "__main__":
    # 1. åŠ è½½æ¨¡å‹
    try:
        model = load_model()
    except Exception as e:
        print(e)
        exit(1)
    
    # 2. æ‰¹é‡é¢„æµ‹æŒ‡å®šæ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰å›¾ç‰‡
    predict_results = batch_predict_from_dir(model, PREDICT_DIR)
    
    # 3. è¾“å‡ºé¢„æµ‹æ±‡æ€»
    if predict_results:
        print("\n" + "="*80)
        print("ğŸ“Š æ‰¹é‡é¢„æµ‹æ±‡æ€»")
        print("="*80)
        # ç»Ÿè®¡ç»“æœ
        success_count = len([r for r in predict_results if r["status"] == "success"])
        error_count = len([r for r in predict_results if r["status"] == "error"])
        book_count = len([r for r in predict_results if r["status"] == "success" and r["is_book"]])
        non_book_count = len([r for r in predict_results if r["status"] == "success" and not r["is_book"]])
        
        print(f"æ€»å›¾ç‰‡æ•°ï¼š{len(predict_results)} | æˆåŠŸé¢„æµ‹ï¼š{success_count} | é¢„æµ‹å¤±è´¥ï¼š{error_count}")
        if success_count > 0:
            print(f"åˆ¤å®šä¸ºå›¾ä¹¦å°é¢ï¼š{book_count} å¼  | åˆ¤å®šä¸ºéå›¾ä¹¦å°é¢ï¼š{non_book_count} å¼ ")
        
        # è¾“å‡ºå¤±è´¥æ¡ˆä¾‹ï¼ˆå¦‚æœæœ‰ï¼‰
        if error_count > 0:
            print("\nâŒ é¢„æµ‹å¤±è´¥çš„å›¾ç‰‡ï¼š")
            for res in predict_results:
                if res["status"] == "error":
                    print(f"  - {res['filename']}ï¼š{res['message']}")
        print("="*80)